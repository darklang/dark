apiVersion: apps/v1
kind: Deployment
metadata:
  name: apiserver-deployment
  annotations:
    kubernetes.io/change-cause: "{CHANGE_CAUSE}"
spec:
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app: apiserver-service
  replicas: 1 # FSTODO
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 0 # FSTODO
  template:
    metadata:
      labels:
        app: apiserver-service
    spec:
      containers:
        - name: apiserver-ctr
          image: "gcr.io/balmy-ground-195100/gcp-fsharp-apiserver:{IMAGE}"
          # Resource limits + requests are intentionally the same, to ensure
          # this pod is a 'Guaranteed' pod, ref:
          #  https://medium.com/google-cloud/quality-of-service-class-qos-in-kubernetes-bb76a89eb2c6
          resources:
            requests:
              memory: "1000Mi"
              cpu: "125m"
            limits:
              memory: "1000Mi"
              cpu: "125m"
          ports:
            # Name length limit is 15 chars.
            - name: apisvr-ctr-port
              containerPort: 9000
          # lifecycle:
          #   preStop:
          #     We implement the SIGTERM handler instead (even if we used preStop we'd
          #     still need to check how SIGTERM works so may as well simplify it to one
          #     concept)
          startupProbe: # has it started? Allows other probes
            httpGet:
              path: /k8s/startupProbe
              port: 6000
            failureThreshold: 24 # kill container after 2 minutes (24x5s checks)
            timeoutSeconds: 10
            periodSeconds: 5
          readinessProbe: # can it serve http requests?
            httpGet:
              path: /k8s/readinessProbe
              port: 6000
            initialDelaySeconds: 0
            periodSeconds: 5
            successThreshold: 3
          livenessProbe: # is it still alive?
            httpGet:
              path: /k8s/livenessProbe
              port: 6000
            initialDelaySeconds: 0
            periodSeconds: 10
            timeoutSeconds: 10
            failureThreshold: 3 # kill container after 30 seconds (3x10s checks)
          envFrom:
            - configMapRef:
                name: gke-dark-prod
          env:
            - name: DARK_CONFIG_RUNNING_IN_GKE
              value: "true"
            - name: DARK_CONFIG_ROLLBAR_POST_SERVER_ITEM
              valueFrom:
                secretKeyRef:
                  name: rollbar
                  key: post_token
            - name: DARK_CONFIG_PUSHER_KEY
              valueFrom:
                secretKeyRef:
                  name: pusher-account-credentials
                  key: key
            # connect to sql proxy in the same pod
            - name: DARK_CONFIG_DB_HOST
              value: 127.0.0.1
            - name: DARK_CONFIG_DB_USER
              valueFrom:
                secretKeyRef:
                  name: cloudsql-db-credentials
                  key: username
            - name: DARK_CONFIG_DB_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: cloudsql-db-credentials
                  key: password
            - name: GOOGLE_APPLICATION_CREDENTIALS_JSON
              valueFrom:
                secretKeyRef:
                  name: dark-static-assets
                  key: balmy-ground-195100-d9b0f3de3013.json

        #########################
        # Legacyserver
        #########################
        - name: legacy-ctr
          image: "gcr.io/balmy-ground-195100/legacyserver:{LEGACYSERVER_IMAGE}"
          # Resource limits + requests are intentionally the same, to ensure
          # this pod is a 'Guaranteed' pod, ref:
          #  https://medium.com/google-cloud/quality-of-service-class-qos-in-kubernetes-bb76a89eb2c6
          resources:
            requests:
              memory: "1000Mi"
              cpu: "125m"
            limits:
              memory: "1000Mi"
              cpu: "125m"
          ports:
            - name: bwd-legacy-port
              containerPort: 5000
          lifecycle:
            preStop:
              httpGet:
                # ???? https://github.com/kubernetes/kubernetes/issues/56770
                path: /k8s/pkill
                port: 5000
          readinessProbe:
            httpGet:
              path: /k8s/readinessProbe
              port: 5000
            initialDelaySeconds: 5
            periodSeconds: 5
          livenessProbe:
            httpGet:
              path: /k8s/livenessProbe
              port: 5000
            # Giving 2 minutes grace here, there's an outstanding k8s issue
            # preventing you from specifying "start checking liveness after an
            # ok from readiness", which is what you'd expect.
            # https://github.com/kubernetes/kubernetes/issues/27114
            initialDelaySeconds: 120
            periodSeconds: 10 # every 10 seconds
            timeoutSeconds: 10 # time out after 10 seconds
            failureThreshold: 3 # kill container after 3 successive time outs
          envFrom:
            - configMapRef:
                name: gke-dark-prod
          env:
            - name: DARK_CONFIG_RUNNING_IN_GKE
              value: "true"
            # FSTODO: make rollbar work in legacyserver
            - name: DARK_CONFIG_ROLLBAR_POST_SERVER_ITEM
              valueFrom:
                secretKeyRef:
                  name: rollbar
                  key: post_token
            # FSTODO: remove unused config
            - name: DARK_CONFIG_PUSHER_KEY
              valueFrom:
                secretKeyRef:
                  name: pusher-account-credentials
                  key: key

        #########################
        # Postgres proxy config
        # To connect to postgres from kubernetes, we need to add a proxy. See
        # https://cloud.google.com/sql/docs/postgres/connect-kubernetes-engine.
        # Note in particular that we needed to create a service account and a
        # set of GKE secrets, listed below, to manage this.
        #########################
        - name: cloudsql-proxy
          image: gcr.io/cloudsql-docker/gce-proxy:1.11
          resources:
            requests:
              memory: "500Mi"
              cpu: "125m"
            limits:
              memory: "500Mi"
              cpu: "125m"
          # https://github.com/GoogleCloudPlatform/cloudsql-proxy/issues/128
          command: ["/bin/sh",
                    "-c",
                    "/cloud_sql_proxy -dir=/cloudsql -instances=balmy-ground-195100:us-west1:{CLOUDSQL_INSTANCE_NAME}=tcp:5432 -credential_file=/secrets/cloudsql/credentials.json"]
          volumeMounts:
            - name: cloudsql-instance-credentials
              mountPath: /secrets/cloudsql
              readOnly: true

        #########################
        # Nginx proxy
        #########################
        - name: apiserver-http-proxy
          image: nginx:1.16.1
          resources:
            requests:
              memory: "50Mi"
              cpu: "100m"
            limits:
              memory: "50Mi"
              cpu: "100m"
          ports:
            - name: apiserver-http-proxy-port
              containerPort: 8000
          volumeMounts:
            - mountPath: /etc/nginx/nginx.conf
              name: base-nginx-conf
              subPath: base-nginx.conf
            - mountPath: /etc/nginx/conf.d
              name: nginx-conf
          lifecycle:
            preStop:
              exec:
                # https://kubernetes.io/docs/tasks/configure-pod-container/attach-handler-lifecycle-event/
                command: ["/bin/sh", "-c", "nginx -s quit; while killall -0 nginx; do sleep 1; done"]

      volumes:
        - name: cloudsql-instance-credentials
          secret:
            secretName: cloudsql-instance-credentials
        - name: base-nginx-conf
          configMap:
            name: nginx-base
        - name: nginx-conf
          configMap:
            name: nginx-apiserver
