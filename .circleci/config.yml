version: 2.1
executors:
  my-executor:
    docker:
      - image: docker:stable-git
  in-container:
    working_directory: ~/app
    environment:
      IN_DEV_CONTAINER: true
    docker:
      - image: 500377317163.dkr.ecr.us-east-1.amazonaws.com/dark-ci:latest


commands:
  show-large-files-and-directories:
    steps:
      - run:
          # show any file or directory over 50M in size
          # note alpine find doesn't support +50M here
          name: show large files and directories
          command: |
            find . -size +51200k -exec du -h {} \;
            du -ht 50M

  ##########################
  # Getting into the remote container
  ##########################
  copy-into-container:
    steps:
      - show-large-files-and-directories
      # https://circleci.com/docs/2.0/building-docker-images/#mounting-folders
      - run:
          name: Copy app directory into dev container
          command: |
            time docker cp . vols:/home/dark/app
      - set-ownership:
          path: "/home/dark/app"

  set-ownership:
    parameters:
      path:
        type: string
    steps:
      # https://circleci.com/docs/2.0/building-docker-images/#mounting-folders
      - run:
          name: Set the ownership of copied files
          command: |
            docker run -i --volumes-from vols alpine sh -c "adduser -D -u 1000 dark; chown -R dark << parameters.path >>"

  ##########################
  # Check the worktree
  ##########################
  assert-clean-worktree:
    steps:
      - run:
          name: Assert the worktree is clean
          command: "bash -c '[[ -z $(git status -s) ]] && echo Workdir is clean || { echo Workdir is not clean:; git status -s; $(exit 1); }'"

  ##########################
  # Rust
  ##########################
  rust-setup:
    parameters:
      project:
        type: string
    steps:
      - restore_cache:
          keys:
            # This cache should be about 500MB or so. It balloons over time and needs to be deleted.
            - v0d-<< parameters.project >>-{{ checksum "<< parameters.project >>/Cargo.lock" }}-{{ .Branch }}
            - v0d-<< parameters.project >>-{{ checksum "<< parameters.project >>/Cargo.lock" }}
            - v0d-<< parameters.project >>

  rust-finish:
    parameters:
      project:
        type: string
    steps:
      - run:
          name: Reduce caches
          command: cargo cache -a
      - show-large-files-and-directories

      # must persist to workspace first, as next step will remove built release artifact
      - persist_to_workspace:
          root: "."
          paths: [ << parameters.project >>/target/release/dark-<< parameters.project >> ]

      # This removes files in the top-level of target/{debug,release}, which include the actual built artifact
      # and other intermediates that will always be rebuilt on the next build (so there's no sense in caching them).
      # It also includes our own (dark*) build files from deps, which are likewise always rebuilt.
      #
      # See https://github.com/rust-lang/cargo/issues/5885 for discussion and details
      - run:
          name: Cleanup frequently changing rust artifacts
          command: |
            find << parameters.project >>/target -maxdepth 2 -type f -delete
            rm -rf << parameters.project >>/target/{debug,release}/{deps,.fingerprint}/dark*

      # https://doc.rust-lang.org/nightly/cargo/guide/cargo-home.html#caching-the-cargo-home-in-ci
      - save_cache:
          name: "Save << parameters.project >> cache"
          paths:
            - << parameters.project >>/target
            - .cargo/bin/
            - .cargo/registry/index/
            - .cargo/registry/cache/
            - .cargo/git/db/
          key: v0d-<< parameters.project >>-{{ checksum "<< parameters.project >>/Cargo.lock" }}-{{ .Branch }}

  ##########################
  # Initializing the containers
  ##########################

  setup-cache-names:
    steps:
      - run:
          name: "Setup cache names"
          command: |
            date +"%F" > rundir/today-timestamp
            date +"%F" -d "today - 1 days" > rundir/minus1-timestamp
            date +"%F" -d "today - 2 days" > rundir/minus2-timestamp
            date +"%F" -d "today - 3 days" > rundir/minus3-timestamp

  prep-container-creation:
    steps:
      - setup_remote_docker:
          docker_layer_caching: true

      # Save the docker env: type .docker-env when sshing in, then you can
      # use ./scripts/run-in-docker
      - run: |
          env \
          | grep 'DOCKER\|NO_PROXY' \
          | sed 's/^/export /' \
          > /root/docker-env

      - run:
          name: Install outer container utilities
          # coreutils for gnu date
          command: apk add --update bash coreutils jq

  initialize:
    steps:
      - setup_remote_docker:
          docker_layer_caching: true

      # Save the docker env: type .docker-env when sshing in, then you can
      # use ./scripts/run-in-docker
      - run: |
          env \
          | grep 'DOCKER\|NO_PROXY' \
          | sed 's/^/export /' \
          > /root/docker-env

      - run:
          name: Prepare dev-container volume (vols)
          command: |
            # We list all the directories because volume creation has to be
            # done on creation afaict.
            docker create -v /home/dark/app -v /home/dark/.esy -v /home/dark/.cargo -v /usr/local/cargo-home --name vols alpine:3.4 /bin/true

      - run:
          name: Install outer container utilities
          # coreutils for gnu date
          command: |
            apk add --update bash coreutils jq
      - setup-cache-names


  ##########################
  # misc
  ##########################
  run-background-container:
    steps:
      - run:
          name: Build container if necessary
          command: scripts/builder
      - run:
          name: Run background container
          command: scripts/builder --ci-serve
          background: true
  wait-for-container:
    steps:
      - run:
          name: "Wait for container"
          command: |
            # --foreground because this is run in a script by circle
            timeout --foreground 10m scripts/wait-until-container-ready
  wait-for-server:
    steps:
      - wait-for-container
      - run:
          name: "Wait for server"
          command: |
            # --foreground because this is run in a script by circle
            timeout --foreground 10m scripts/wait-until-server-ready
  auth-with-gcp:
    steps:
      - run:
          name: Auth with GCP
          command: |
            echo $GCLOUD_SERVICE_KEY | base64 --decode --ignore-garbage > gcloud-service-key.json
            docker cp gcloud-service-key.json dark-dev:/home/dark/app/gcloud-service-key.json
            scripts/run-in-docker gcloud auth activate-service-account --key-file /home/dark/app/gcloud-service-key.json

##########################
# Actual workflow
##########################
jobs:

  build-container:
    executor: my-executor
    steps:
      - checkout
      - prep-container-creation
      - run: apk add python3 --update-cache 
      - run: pip3 install awscli --upgrade --user
      - run: $(/root/.local/bin/aws ecr get-login --no-include-email --region us-east-1)
      - run:
          name: Pre-fetch docker container
          shell: bash
          command: |
            if [[ `docker images dark-ci -q` == "" ]]
            then docker pull 500377317163.dkr.ecr.us-east-1.amazonaws.com/dark-ci:latest
            fi
      - run: docker build -t dark-ci .
      - run: docker tag dark-ci:latest 500377317163.dkr.ecr.us-east-1.amazonaws.com/dark-ci:latest
      - run: docker push 500377317163.dkr.ecr.us-east-1.amazonaws.com/dark-ci:latest

  build-client:
    executor: in-container
    steps:
      - checkout
      - run: scripts/support/setup-circleci-environment
      - run: scripts/support/create-cache-directories
      - restore_cache:
          keys:
            - v0d-client-{{ checksum "client/yarn.lock" }}-{{ .Branch }}
            - v0d-client-{{ checksum "client/yarn.lock" }}
            - v0d-client
      - run: scripts/support/compile --test client/package.json client/src/styles/app.scss 
      - assert-clean-worktree
      - run: scripts/support/shellchecker # run here cause its the fastest
      - show-large-files-and-directories
      - save_cache:
          name: "Save packagejson-specific cache"
          paths: [ "client/node_modules" ]
          key: v0d-client-{{ checksum "client/yarn.lock" }}-{{ .Branch }}
      - persist_to_workspace:
          root: "."
          paths:
            # Just enough for integration tests and deploy
            - backend/static/app.css
            - backend/static/app.js
            - backend/static/appsupport.js
            - backend/static/fetcher.js
            - backend/static/analysiswrapper.js
      - run:
          name: Prepare artifacts
          when: always
          command: |
            cp backend/static/etags.json rundir
            ls -la backend/static > rundir/asset-list.txt
      - store_artifacts:
          path: /home/dark/app/rundir
      - store_test_results:
          path: rundir/test_results

  build-backend:
    executor: in-container
    steps:
      - checkout
      - run: scripts/support/setup-circleci-environment
      - run: scripts/support/create-cache-directories
      - restore_cache:
          keys:
            - v0d-backend-{{ checksum "backend/esy.json" }}-{{ .Branch }}
            - v0d-backend-{{ checksum "backend/esy.json" }}
            - v0d-backend
      # appsupport is needed for a unit test
      - run: touch backend/static/appsupport.js
      - show-large-files-and-directories
      - run: scripts/support/compile --test backend/esy.json
      - assert-clean-worktree
      # Doesn't need to be run post-build, but takes <1s to run
      - run: scripts/ocaml-find-unused backend/test
      - run:
          name: Reduce size of esy cache
          command: rm -Rf /home/dark/.esy/3/b
      - show-large-files-and-directories
      - save_cache:
          name: "Save daily backend cache"
          paths:
            - backend/_build
            - backend/_esy
            - backend/node_modules
            - .esy
          key: v0d-backend-{{ checksum "backend/esy.json" }}-{{ .Branch }}
      - persist_to_workspace:
          root: "."
          paths:
            # Just enough for integration tests and deploy
            - backend/_build/default/bin/server.exe
            - backend/_build/default/bin/queue_worker.exe
            - backend/_build/default/bin/cron_checker.exe
            - backend/static/analysis.js
      - store_artifacts:
          path: rundir
      - store_test_results:
          path: rundir/test_results


  build-stroller:
    executor: in-container
    steps:
      - checkout
      - run: scripts/support/setup-circleci-environment
      - run: scripts/support/create-cache-directories
      - rust-setup: { project: "stroller" }
      - run: scripts/support/compile stroller/Cargo.toml --test
      - run: scripts/gcp-build-containers --skip-ocaml --skip-queue-scheduler
      - assert-clean-worktree
      - rust-finish: { project: "stroller" }

  build-queue-scheduler:
    executor: in-container
    steps:
      - checkout
      - run: scripts/support/setup-circleci-environment
      - run: scripts/support/create-cache-directories
      - rust-setup: { project: "queue-scheduler" }
      - run: scripts/support/compile queue-scheduler/Cargo.toml --test
      - run: scripts/gcp-build-containers --skip-ocaml --skip-stroller
      - assert-clean-worktree
      - rust-finish: { project: "queue-scheduler" }

  build-postgres-honeytail:
    executor: my-executor
    steps:
      - checkout
      - prep-container-creation
      - run: cd postgres-honeytail && docker build -t dark-gcp-postgres-honeytail .

  validate-honeycomb-config:
    executor: my-executor
    steps:
      - checkout
      - prep-container-creation
      - run: apk add --update bash python py-pip && pip install yq
      - run:
          shell: bash
          command: scripts/support/test-honeycomb-config.sh

  integration-tests:
    executor: in-container
    parallelism: 4
    steps:
      - checkout
      - run: scripts/support/setup-circleci-environment
      - run: scripts/support/create-cache-directories
      - attach_workspace: { at: "." }
      - restore_cache: # get testcafe
          keys:
            - v0d-client-{{ checksum "client/yarn.lock" }}-{{ .Branch }}
            - v0d-client-{{ checksum "client/yarn.lock" }}
            - v0d-client
      - show-large-files-and-directories
      - run:
          name: Run integration tests
          shell: bash
          command: |
            # get full list of tests
            grep ^test integration-tests/tests.js | sed 's/.*"\(.*\)".*/\1/' > rundir/all-tests
            # split them using timing info
            TESTS=$(circleci tests split --split-by=timings --timings-type=testname rundir/all-tests)
            # concat them into a pattern (note: $TESTS is deliberately unquoted)
            PATTERN=$(printf -- "^%s$|" $TESTS)
            # remove last char
            PATTERN=${PATTERN%?}
            integration-tests/run.sh --pattern="$PATTERN"
      - assert-clean-worktree
      - store_artifacts:
          path: rundir
      - store_test_results:
          path: rundir/test_results


  rust-integration-tests:
    executor: in-container
    steps:
      - checkout
      - run: scripts/support/setup-circleci-environment
      - run: scripts/support/create-cache-directories
      - attach_workspace: { at: "." }
      - show-large-files-and-directories
      - rust-setup: { project: "queue-scheduler" }
      - run:
          name: Run queue-scheduler tests
          command: scripts/run-rust-tests queue-scheduler
      - assert-clean-worktree


  push-to-gcp:
    executor: my-executor
    steps:
      - checkout
      - initialize
      - attach_workspace: { at: "." }
      - show-large-files-and-directories
      - copy-into-container
      - run-background-container
      # Now that the workspaces have combined, we need to regenerate etags.json
      - wait-for-server
      - run: docker cp vols:/home/dark/app/backend/static/etags.json backend/static
      - run: scripts/run-in-docker scripts/gcp-build-containers
      - auth-with-gcp
      - run: scripts/run-in-docker scripts/push-assets-to-cdn
      - run: scripts/run-in-docker scripts/gcp-push-images-to-gcr
      # Save the image IDs for deployment later
      - run: |
          set -x
          mkdir gcr-image-ids
          time docker images gcr.io/balmy-ground-195100/dark-gcp -q | head -n 1 > gcr-image-ids/server
          time docker images gcr.io/balmy-ground-195100/dark-gcp-qw -q | head -n 1 > gcr-image-ids/qw
          time docker images gcr.io/balmy-ground-195100/dark-gcp-cron -q | head -n 1 > gcr-image-ids/cron
          time docker images gcr.io/balmy-ground-195100/dark-gcp-stroller -q | head -n 1 > gcr-image-ids/stroller
          time docker images gcr.io/balmy-ground-195100/dark-gcp-queue-scheduler -q | head -n 1 > gcr-image-ids/queue-scheduler
          time docker images gcr.io/balmy-ground-195100/tunnel -q | head -n 1 > gcr-image-ids/tunnel
          time docker images gcr.io/balmy-ground-195100/dark-gcp-postgres-honeytail -q | head -n 1 > gcr-image-ids/postgres-honeytail
      - persist_to_workspace:
          root: "."
          paths: [ gcr-image-ids ]


  deploy:
    executor: my-executor
    steps:
      - checkout
      - initialize
      - attach_workspace: { at: "." }
      - show-large-files-and-directories
      - copy-into-container
      - run-background-container
      - wait-for-container
      - auth-with-gcp
      - run: |
          scripts/run-in-docker scripts/gke-deploy       \
            --server-image-id=`cat gcr-image-ids/server` \
            --qw-image-id=`    cat gcr-image-ids/qw`     \
            --cron-image-id=`  cat gcr-image-ids/cron`   \
            --stroller-image-id=`cat gcr-image-ids/stroller` \
            --queue-scheduler-image-id=`cat gcr-image-ids/queue-scheduler` \
            --tunnel-image-id=`cat gcr-image-ids/tunnel` \
            --postgres-honeytail-image-id=`cat gcr-image-ids/postgres-honeytail`


workflows:
  version: 2
  build-and-deploy:
    jobs:
      # initial builds & tests
      - build-container
      - build-postgres-honeytail
      - validate-honeycomb-config

      - build-backend: { requires: [ build-container ] }
      - build-client: { requires: [ build-container ] }
      - build-stroller: { requires: [ build-container ] }
      - build-queue-scheduler: { requires: [ build-container ] }

      # expensive tests
      - rust-integration-tests:
          requires:
            - build-backend
            - build-client
            - build-queue-scheduler
      - integration-tests:
          requires:
            - build-backend
            - build-client

      # pre-deploy, in parallel with integration-tests
      - push-to-gcp:
          filters:
            branches:
              only: master
          requires:
            - build-backend
            - build-client
            - build-stroller
            - build-queue-scheduler
            - build-postgres-honeytail

      # actual deploy
      - deploy:
          filters:
            branches:
              only: master
          requires:
            - validate-honeycomb-config
            - integration-tests
            - rust-integration-tests
            - push-to-gcp
