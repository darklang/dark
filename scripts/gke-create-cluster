#!/usr/bin/env bash

. ./scripts/support/assert-in-container "$0" "$@"

set -euo pipefail

# Defaults
DARK_CLUSTER_REGION="us-west1"
DARK_CLUSTER_PROJECT="balmy-ground-195100"
DARK_CLUSTER="darkcluster$(date +"%s")"
DARK_CLUSTER_NUM_NODES="1"
DARK_CLUSTER_VERSION="1.15.9-gke.24" # available versions: `gcloud container get-server-config --region us-west1`
DARK_CLUSTER_CONFIGMAP_FILE="config/gke-builtwithdark"
CLOUDSQL_INSTANCE_NAME="dark-west"
DARK_CLUSTER_NODE_MACHINE_TYPE="n1-standard-8"

HELP="$(cat <<EOF
Usage: $0 REQUIRED_ARGS [OPTIONS]

A script that deploys a Dark-production-like cluster to Google Kubernetes Engine.

If you're tempted to move the static IPs from the old cluster to a new one generate
with this script, try it out a few times outside of production first. Moving the
static IPs the obvious way is flaky and seems nondeterministic. You may have better
luck swapping DNS if you need it and can live with the long propagation times.

Required:
  --with-new-database=(true|false)  Also create new CloudSQL Postgres instance and wire it up

Secrets:

NOTE: All secrets default to copying the credentials from the current cluster as per
their suggestion.

  --bwd-tls-crt=...      A base64-encoded TLS .crt file to use for the TLS-enabled ingress
                         serving darklang.com.
                         You can swipe creds from the current cluster with something like
                         the following:
                           kubectl get secrets bwd-tls -o json | jq -r '.data["tls.crt"]'
  --bwd-tls-key=...      A base64-encoded TLS .key file to use for the TLS-enabled ingress
                         serving builtwithdark.com.
                         You can swipe creds from the current cluster with something like
                         the following:
                           kubectl get secrets bwd-tls -o json | jq -r '.data["tls.key"]'
  --dl-tls-crt=...       A base64-encoded TLS .crt file to use for the TLS-enabled ingress
                         serving darklang.com.
                         You can swipe creds from the current cluster with something like
                         the following:
                           kubectl get secrets darklang-tls -o json | jq -r '.data["tls.crt"]'
  --dl-tls-key=...       A base64-encoded TLS .key file to use for the TLS-enabled ingress
                         serving darklang.com.
                         You can swipe creds from the current cluster with something like
                         the following:
                           kubectl get secrets darklang-tls -o json | jq -r '.data["tls.key"]'
  --cloudsql-user=...    The base64 encoded username for the cloudsql DB. You can swipe creds
                         from the current cluster with the following:
                          kubectl get secrets cloudsql-db-credentials -o json \
                            | jq -r '.data["username"]'
  --cloudsql-pass=...    The base64 encoded password for the cloudsql DB. You can swipe creds
                         from the current cluster with the following:
                          kubectl get secrets cloudsql-db-credentials -o json \
                            | jq -r '.data["password"]'

                         NOTE: If --with-new-database is passed this should NOT be base64 encoded.
                         We're sorry.
  --cloudsql-creds=...   A base64-encoded JSON document including credentials for the cloudsql
                         instance. Specifically, this is the token for the service account that the
                         sidecar CloudSQL proxy uses to to communicate with CloudSQL.
                         You can swipe creds from the current cluster with something
                         like the following:
                           kubectl get secret cloudsql-instance-credentials -o json \
                             | jq -r '.data["credentials.json"]'
  --assets-creds=...     A base64-encoded JSON document including credentials for uploading
                         to the dark-static-assets bucket. You can swipe creds from the
                         current cluster with something like the following:
                           kubectl get secret dark-static-assets -o json \
                             | jq -r '.data["balmy-ground-195100-d9b0f3de3013.json"]'

Other Options:

  --region=...           The Google cloud region to deploy into (default $DARK_CLUSTER_REGION).
  --project=...          The Google cloud project to deploy into (default $DARK_CLUSTER_PROJECT).
  --cluster=...          The name of the new cluster (default is to append unix time to
                         'darkcluster', a la $DARK_CLUSTER).
  --num-nodes=...        The number of nodes *per zone* in the cluster. GKE default is to deploy
                         to three zones, so the actual number of deployed nodes will be 3x this
                         amount (default $DARK_CLUSTER_NUM_NODES).
  --machine-type=...     The machine type of the nodes created. (default $DARK_CLUSTER_NODE_MACHINE_TYPE).
  --configmap-file=...   The path of the env-file the cluster config map will be created from/updated
                         to (default $DARK_CLUSTER_CONFIGMAP_FILE)
  --version=...          The Kubernetes version to run on the master and nodes. New versions are
                         released and old versions are obsoleted on GKE quickly, so you will
                         probably need to find the newest version (default $DARK_CLUSTER_VERSION).
  --help                 Display this menu.

EOF
)"


for i in "$@"
do
  case "${i}" in
    --region=*)
      DARK_CLUSTER_REGION="${i/--region=/''}"
      ;;
    --project=*)
      DARK_CLUSTER_PROJECT="${i/--project=/''}"
      ;;
    --cluster=*)
      DARK_CLUSTER="${i/--cluster=/''}"
      ;;
    --num-nodes=*)
      DARK_CLUSTER_NUM_NODES="${i/--num-nodes=/''}"
      ;;
    --configmap-file=*)
      DARK_CLUSTER_CONFIGMAP_FILE="${i/--configmap-file=/''}"
      ;;
    --version=*)
      DARK_CLUSTER_VERSION="${i/--version=/''}"
      ;;
    --cloudsql-user=*)
      DARK_CLUSTER_CLOUDSQL_USER="${i/--cloudsql-user=/''}"
      ;;
    --cloudsql-pass=*)
      DARK_CLUSTER_CLOUDSQL_PASS="${i/--cloudsql-pass=/''}"
      ;;
    --bwd-tls-crt=*)
      DARK_CLUSTER_BWD_TLS_CRT="${i/--bwd-tls-crt=/''}"
      ;;
    --bwd-tls-key=*)
      DARK_CLUSTER_BWD_TLS_KEY="${i/--bwd-tls-key=/''}"
      ;;
    --dl-tls-crt=*)
      DARK_CLUSTER_DL_TLS_CRT="${i/--dl-tls-crt=/''}"
      ;;
    --dl-tls-key=*)
      DARK_CLUSTER_DL_TLS_KEY="${i/--dl-tls-key=/''}"
      ;;
    --cloudsql-creds=*)
      DARK_CLUSTER_CLOUDSQL_INSTANCE_CREDS="${i/--cloudsql-creds=/''}"
      ;;
    --assets-creds=*)
      DARK_CLUSTER_STATIC_ASSETS_CREDS="${i/--assets-creds=/''}"
      ;;
    --with-new-database=*)
      WITH_NEW_DATABASE="${i/--with-new-database=/''}"
      ;;
    --machine-type=*)
      DARK_CLUSTER_NODE_MACHINE_TYPE="${i/--machine-type=/''}"
      ;;
    --honeycomb-write-key=*)
      DARK_CLUSTER_HONEYCOMB_WRITE_KEY="${i/--honeycomb-write-key=/''}"
      DARK_CLUSTER_HONEYCOMB_WRITE_KEY=$(printf "%s" "$DARK_CLUSTER_HONEYCOMB_WRITE_KEY" | base64)
      ;;
    --help)
      echo "$HELP"
      exit 0
      ;;
    *)
      echo "Unexpected argument: $i"
      echo "$HELP"
      exit 1
      ;;
  esac
done

function print_step () {
  tput setab 6 && echo "=>" "$@" && tput sgr0
}

# validate DARK_CLUSTER_VERSION
if ( ! grep "${DARK_CLUSTER_VERSION}" \
      <(gcloud container get-server-config --region "${DARK_CLUSTER_REGION}" 2>/dev/null\
        | sed -n '/validMasterVersions:/,/validNodeVersions:/p' \
        | head -n-1 \
        | tail -n +2 \
        | sed 's/^- //'));
then
  echo "${DARK_CLUSTER_VERSION} is not currently supported in ${DARK_CLUSTER_REGION}."
  echo "The prod cluster is currently running $(kubectl get nodes \
    | grep -v VERSION \
    | awk '{print $5}' \
    | sed 's/^v//' \
    | head -n 1), and currently-supported versions for new clusters are:"
  gcloud container get-server-config --region "${DARK_CLUSTER_REGION}" 2>/dev/null \
    | sed -n '/validMasterVersions:/,/validNodeVersions:/p' \
    | head -n-1 \
    | tail -n +2
  exit 1;
fi

if [ ! -v WITH_NEW_DATABASE ]; then
  echo "--with-new-database is required"
  echo "$HELP"
  exit 1
elif [[ "${WITH_NEW_DATABASE}" == "true" ]]; then
  # currently we use the default user
  DARK_CLUSTER_CLOUDSQL_USER=$(printf "%s" "postgres" | base64)
  CLOUDSQL_INSTANCE_NAME="$DARK_CLUSTER"
  if [ ! -v DARK_CLUSTER_CLOUDSQL_PASS ]; then
    echo "When creating a new database with a cluster, --cloudsql-pass is required"
    exit 1
  fi
  ./scripts/gcp-create-db \
    --region=$DARK_CLUSTER_REGION \
    --cloudsql-instance-name=$CLOUDSQL_INSTANCE_NAME \
    --cloudsql-pass=$DARK_CLUSTER_CLOUDSQL_PASS

  DARK_CLUSTER_CLOUDSQL_PASS=$(printf "%s" "$DARK_CLUSTER_CLOUDSQL_PASS" | base64)
elif [[ "${WITH_NEW_DATABASE}" == "false" ]]; then
  true
else
  echo "--with-new-database must be either true or false"
  echo "$HELP"
  exit 1
fi
# This is usually the "original" (prod) cluster, though plausibly you could be
# trying to clone a one non-prod cluster
ORIG_CLUSTER=$(< current-cluster)
# This function is primarily for www.hellobirb.com-tls, and the coming kiksht
# cert, and any other we set up before we automate cert management
function copy_tls_secret_between_clusters {
  name=$1
  # set orig cluster
  gcloud container clusters get-credentials "projects/${DARK_CLUSTER_PROJECT}/zones/${DARK_CLUSTER_REGION}/clusters/${ORIG_CLUSTER}" --region=${DARK_CLUSTER_REGION}
  print_step "Fetching TLS key & cert for $name"
  TLS_KEY="$(kubectl get secrets "$name" -o json | jq -r '.data["tls.key"]')"
  TLS_CRT="$(kubectl get secrets "$name" -o json | jq -r '.data["tls.crt"]')"

  # set new cluster
  gcloud container clusters get-credentials "projects/${DARK_CLUSTER_PROJECT}/zones/${DARK_CLUSTER_REGION}/clusters/${DARK_CLUSTER}" --region=${DARK_CLUSTER_REGION}
  print_step "Putting TLS key & cert for $name into new cluster"
  kubectl create -f - <<EOF
  {
    "apiVersion": "v1",
    "kind": "Secret",
    "type": "kubernetes.io/tls",
    "data": {
      "tls.crt": "$TLS_CRT",
      "tls.key": "$TLS_KEY"
    },
    "metadata": {
          "name": "$name"
    }
  }
EOF
}

if [ ! -v DARK_CLUSTER_BWD_TLS_KEY ] || [ ! -v DARK_CLUSTER_BWD_TLS_CRT ] \
     || [ ! -v DARK_CLUSTER_DL_TLS_KEY ] || [ ! -v DARK_CLUSTER_DL_TLS_CRT ] \
     || [ ! -v DARK_CLUSTER_CLOUDSQL_INSTANCE_CREDS ] || [ ! -v DARK_CLUSTER_CLOUDSQL_USER ] \
     || [ ! -v DARK_CLUSTER_HONEYCOMB_WRITE_KEY ] \
     || [ ! -v DARK_CLUSTER_CLOUDSQL_PASS ]; then
  print_step "authorizing with the current cluster"
  gcloud container clusters get-credentials "projects/${DARK_CLUSTER_PROJECT}/zones/${DARK_CLUSTER_REGION}/clusters/$(< current-cluster)" --region=${DARK_CLUSTER_REGION}

  if [ ! -v DARK_CLUSTER_BWD_TLS_KEY ]; then
    print_step "Fetching DARK_CLUSTER_BWD_TLS_KEY"
    DARK_CLUSTER_BWD_TLS_KEY=$(kubectl get secrets bwd-tls -o json | jq -r '.data["tls.key"]')
  fi

  if [ ! -v DARK_CLUSTER_BWD_TLS_CRT ]; then
    print_step "Fetching DARK_CLUSTER_BWD_TLS_CRT"
    DARK_CLUSTER_BWD_TLS_CRT=$(kubectl get secrets bwd-tls -o json | jq -r '.data["tls.crt"]')
  fi

  if [ ! -v DARK_CLUSTER_DL_TLS_KEY ]; then
    print_step "Fetching DARK_CLUSTER_DL_TLS_KEY"
    DARK_CLUSTER_DL_TLS_KEY=$(kubectl get secrets darklang-tls -o json | jq -r '.data["tls.key"]')
  fi

  if [ ! -v DARK_CLUSTER_DL_TLS_CRT ]; then
    print_step "Fetching DARK_CLUSTER_DL_TLS_CRT"
    DARK_CLUSTER_DL_TLS_CRT=$(kubectl get secrets darklang-tls -o json | jq -r '.data["tls.crt"]')
  fi

  if [ ! -v DARK_CLUSTER_CLOUDSQL_USER ]; then
    print_step "Fetching DARK_CLUSTER_CLOUDSQL_USER"
    DARK_CLUSTER_CLOUDSQL_USER=$(kubectl get secret cloudsql-db-credentials -o json | jq -r '.data["username"]')
  fi

  if [ ! -v DARK_CLUSTER_CLOUDSQL_PASS ]; then
    print_step "Fetching DARK_CLUSTER_CLOUDSQL_PASS"
    DARK_CLUSTER_CLOUDSQL_PASS=$(kubectl get secret cloudsql-db-credentials -o json | jq -r '.data["password"]')
  fi

  if [ ! -v DARK_CLUSTER_CLOUDSQL_INSTANCE_CREDS ]; then
    print_step "Fetching DARK_CLUSTER_CLOUDSQL_INSTANCE_CREDS"
    DARK_CLUSTER_CLOUDSQL_INSTANCE_CREDS=$(kubectl get secret cloudsql-instance-credentials -o json | jq -r '.data["credentials.json"]')
  fi

  if [ ! -v DARK_CLUSTER_STATIC_ASSETS_CREDS ]; then
    print_step "Fetching DARK_CLUSTER_STATIC_ASSETS_CREDS"
    DARK_CLUSTER_STATIC_ASSETS_CREDS=$(kubectl get secret dark-static-assets -o json | jq -r '.data["balmy-ground-195100-d9b0f3de3013.json"]')
  fi

  if [ ! -v DARK_CLUSTER_HONEYCOMB_WRITE_KEY ]; then
    print_step "Fetching DARK_CLUSTER_HONEYCOMB_WRITE_KEY"
    DARK_CLUSTER_HONEYCOMB_WRITE_KEY=$(kubectl get secret honeycomb-writekey -o json | jq -r '.data["key"]')
  fi
fi


print_step "starting a new cluster named $DARK_CLUSTER"

gcloud beta container clusters create "$DARK_CLUSTER" \
  "--num-nodes=${DARK_CLUSTER_NUM_NODES}" \
  "--machine-type=${DARK_CLUSTER_NODE_MACHINE_TYPE}" \
  "--zone=${DARK_CLUSTER_REGION}" \
  "--project=${DARK_CLUSTER_PROJECT}" \
  "--cluster-version=${DARK_CLUSTER_VERSION}" \
  --enable-stackdriver-kubernetes \
  --enable-autorepair \
  --enable-network-policy

print_step "getting creds for the new cluster"
# It's very important you don't forget to switch the kube context here!!!
gcloud container clusters get-credentials "projects/${DARK_CLUSTER_PROJECT}/zones/${DARK_CLUSTER_REGION}/clusters/${DARK_CLUSTER}" --region=${DARK_CLUSTER_REGION}

print_step "installing secrets"
# add tls secrets
kubectl create -f - <<EOF
{
  "apiVersion": "v1",
  "kind": "Secret",
  "type": "kubernetes.io/tls",
  "data": {
    "tls.crt": "$DARK_CLUSTER_BWD_TLS_CRT",
    "tls.key": "$DARK_CLUSTER_BWD_TLS_KEY"
  },
  "metadata": {
        "name": "bwd-tls"
  }
}
EOF
kubectl create -f - <<EOF
{
  "apiVersion": "v1",
  "kind": "Secret",
  "type": "kubernetes.io/tls",
  "data": {
    "tls.crt": "$DARK_CLUSTER_DL_TLS_CRT",
    "tls.key": "$DARK_CLUSTER_DL_TLS_KEY"
  },
  "metadata": {
        "name": "darklang-tls"
  }
}
EOF

kubectl create -f - <<EOF
{
  "apiVersion": "v1",
  "kind": "Secret",
  "type": "Opaque",
  "data": {
    "username": "$DARK_CLUSTER_CLOUDSQL_USER",
    "password": "$DARK_CLUSTER_CLOUDSQL_PASS"
  },
  "metadata": {
    "name": "cloudsql-db-credentials"
  }
}
EOF

# cloudsql instance creds
kubectl create -f - <<EOF
{
  "apiVersion": "v1",
  "kind": "Secret",
  "type": "Opaque",
  "data": {
    "credentials.json": "$DARK_CLUSTER_CLOUDSQL_INSTANCE_CREDS"
  },
  "metadata": {
    "name": "cloudsql-instance-credentials"
  }
}
EOF

# Honeycomb write key
kubectl create -f - <<EOF
{
  "apiVersion": "v1",
  "kind": "Secret",
  "type": "Opaque",
  "data": {
    "key": "$DARK_CLUSTER_HONEYCOMB_WRITE_KEY"
  },
  "metadata": {
    "name": "honeycomb-writekey"
  }
}
EOF

# Honeycomb write key for honeycomb-heapster
kubectl create --namespace=kube-system -f - <<EOF
{
  "apiVersion": "v1",
  "kind": "Secret",
  "type": "Opaque",
  "data": {
    "key": "$DARK_CLUSTER_HONEYCOMB_WRITE_KEY"
  },
  "metadata": {
    "name": "honeycomb-writekey"
  }
}
EOF

kubectl create -f - <<EOF
{
  "apiVersion": "v1",
  "kind": "Secret",
  "type": "Opaque",
  "data": {
    "balmy-ground-195100-d9b0f3de3013.json": "$DARK_CLUSTER_STATIC_ASSETS_CREDS"
  },
  "metadata": {
    "name": "dark-static-assets"
  }
}
EOF

# Copy over the non-bwd, non-darklang tls secrets
print_step "Copy over additional tls secrets"
# Make sure we're in the "old" cluster
gcloud container clusters get-credentials "projects/${DARK_CLUSTER_PROJECT}/zones/${DARK_CLUSTER_REGION}/clusters/${ORIG_CLUSTER}" --region=${DARK_CLUSTER_REGION}
for i in $(kubectl get secrets | grep tls | grep -v -e bwd -e darklang | awk '{print $1}'); do
  print_step "secret name: $i"
  copy_tls_secret_between_clusters "$i"
done
# Make sure we're in the "new" cluster
gcloud container clusters get-credentials "projects/${DARK_CLUSTER_PROJECT}/zones/${DARK_CLUSTER_REGION}/clusters/${DARK_CLUSTER}" --region=${DARK_CLUSTER_REGION}

kubectl create clusterrolebinding circleci-admin-binding \
--clusterrole=cluster-admin \
--user=circleci-deployer@${DARK_CLUSTER_PROJECT}.iam.gserviceaccount.com

print_step "deploying code and containers"

./scripts/gke-deploy \
  "--region=${DARK_CLUSTER_REGION}" \
  "--project=${DARK_CLUSTER_PROJECT}" \
  "--cluster=${DARK_CLUSTER}" \
  "--configmap-file=${DARK_CLUSTER_CONFIGMAP_FILE}" \
  "--cloudsql-instance-name=${CLOUDSQL_INSTANCE_NAME}"

print_step "waiting for a new ip..."

NEW_BWD_IP=null
while [ "$NEW_BWD_IP" == "null" ]; do
  NEW_BWD_IP="$(kubectl get Ingress bwd-tls-ingress -o json | jq -r '.status.loadBalancer.ingress[0].ip')"
done

NEW_DL_IP=null
while [ "$NEW_DL_IP" == "null" ]; do
  NEW_DL_IP="$(kubectl get Ingress darklang-tls-ingress -o json | jq -r '.status.loadBalancer.ingress[0].ip')"
done

print_step "your new cluster is deployed!"
cat <<EOF
Your new IPs are $(tput rev)$NEW_BWD_IP$(tput sgr0) (for builtwithdark) and
$(tput rev)$NEW_DL_IP$(tput sgr0) (for darklang). Wait a little for the
containers to come up...

   watch -d -n 0.2 kubectl get pods

and check it out one of the following ways:

  + edit your /etc/hosts to point builtwithdark.com and *.builtwithdark.com
    to that IP

    $NEW_BWD_IP		builtwithdark.com
    $NEW_BWD_IP	  some-subdomain.builtwithdark.com
    $NEW_DL_IP		darklang.com
    $NEW_DL_IP		static.darklang.com

  + use --resolve in 'curl', e.g.

    curl --resolve "builtwithdark.com:443:$NEW_BWD_IP" https://builtwithdark.com
    curl --resolve "darklang.com:443:$NEW_DL_IP" https://darklang.com

If you're tempted to move the static IPs from the old cluster to this
one, try it out a few times outside of production first. Moving the
static IPs the obvious way is flaky and seems nondeterministic. You
may have better luck swapping DNS if you need it and can live with the
long propagation times.
EOF
