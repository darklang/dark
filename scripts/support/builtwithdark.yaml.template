#apiVersion: apps/v1 - dunno why this doesnt work
apiVersion: apps/v1beta1
kind: Deployment
metadata:
  name: bwd-deployment
  annotations:
    kubernetes.io/change-cause: {CHANGE_CAUSE}
spec:
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app: bwd-app
  replicas: 12
  template:
    metadata:
      labels:
        app: bwd-app
    spec:
      containers:
        - name: bwd-ctr
          image: "gcr.io/balmy-ground-195100/dark-gcp:{IMAGE}"
          ports:
            - name: bwd-ctr-port
              containerPort: 80
          lifecycle:
            preStop:
              httpGet:
                # ???? https://github.com/kubernetes/kubernetes/issues/56770
                path: pkill
                port: 80
          readinessProbe:
            httpGet:
              path: /ready
              port: 80
            initialDelaySeconds: 5
            periodSeconds: 5
          livenessProbe:
            httpGet:
              path: /
              port: 80
            # Giving 2 minutes grace here, there's an outstanding k8s issue
            # preventing you from specifying "start checking liveness after an
            # ok from readiness", which is what you'd expect.
            # https://github.com/kubernetes/kubernetes/issues/27114
            initialDelaySeconds: 120
            periodSeconds: 10 # every 10 seconds
            timeoutSeconds: 10 # time out after 10 seconds
            failureThreshold: 3 # kill container after 3 successive time outs
          envFrom:
            - configMapRef:
                name: gke-dark-prod
          env:
            - name: DARK_CONFIG_RUNNING_IN_GKE
              value: "true"

#########################
# Postgres proxy config
# To connect to postgres from kubernetes, we need to add a proxy. See
# https://cloud.google.com/sql/docs/postgres/connect-kubernetes-engine.
# Note in particular that we needed to create a service account and a
# set of GKE secrets, listed below, to manage this.
#########################
            # connect to sql proxy in the same pod
            - name: DARK_CONFIG_DB_HOST
              value: 127.0.0.1
            - name: DARK_CONFIG_DB_USER
              valueFrom:
                secretKeyRef:
                  name: cloudsql-db-credentials
                  key: username
            - name: DARK_CONFIG_DB_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: cloudsql-db-credentials
                  key: password
            - name: GOOGLE_APPLICATION_CREDENTIALS_JSON
              valueFrom:
                secretKeyRef:
                  name: dark-static-assets
                  key: balmy-ground-195100-d9b0f3de3013.json
        - name: cloudsql-proxy
          image: gcr.io/cloudsql-docker/gce-proxy:1.11
          # https://github.com/GoogleCloudPlatform/cloudsql-proxy/issues/128
          command: ["/bin/sh",
                    "-c",
                    "/cloud_sql_proxy -dir=/cloudsql -instances=balmy-ground-195100:us-west1:dark-west=tcp:5432 -credential_file=/secrets/cloudsql/credentials.json"]
          volumeMounts:
            - name: cloudsql-instance-credentials
              mountPath: /secrets/cloudsql
              readOnly: true

#########################
# Stroller (push proxy) config
#########################
        - name: stroller
          image: "gcr.io/balmy-ground-195100/dark-gcp-stroller:{STROLLER_IMAGE}"
          envFrom:
            - configMapRef:
                name: gke-dark-prod
          lifecycle:
            preStop:
              httpGet:
                path: pkill
                port: 3001
          readinessProbe:
            httpGet:
              path: /
              port: 3001
            initialDelaySeconds: 5
            periodSeconds: 5
          livenessProbe:
            httpGet:
              path: /
              port: 3001
            # Giving 2 minutes grace here, there's an outstanding k8s issue
            # preventing you from specifying "start checking liveness after an
            # ok from readiness", which is what you'd expect.
            # https://github.com/kubernetes/kubernetes/issues/27114
            initialDelaySeconds: 120
            periodSeconds: 10 # every 10 seconds
            timeoutSeconds: 10 # time out after 10 seconds
            failureThreshold: 3 # kill container after 3 successive time outs

        - name: http-proxy
          image: nginx:1.15.3
          ports:
            - name: http-proxy-port
              containerPort: 8000
          volumeMounts:
            - mountPath: /etc/nginx/conf.d
              name: nginx-conf
      volumes:
        - name: cloudsql-instance-credentials
          secret:
            secretName: cloudsql-instance-credentials
        - name: nginx-conf
          configMap:
            name: nginx

#########################
# End postgres proxy config
#########################
---
kind: Service
apiVersion: v1
metadata:
  name: bwd-nodeport
spec:
  type: NodePort
  selector:
    app: bwd-app
  ports:
    - name: bwd-nodeport-port
      protocol: TCP
      port: 80
      targetPort: http-proxy-port
---
apiVersion: extensions/v1beta1
kind: NetworkPolicy
metadata:
  name: bwd-network-policy
spec:
  podSelector:
    matchLabels:
      app: bwd-app
  policyTypes:
  - Egress
  - Ingress
  egress:
  - to:
    - ipBlock:
        cidr: 0.0.0.0/0
        except:
        # make sure to block link-local addresses,
        # since there's sensitive data in http://metadata
        - 169.254.0.0/16
  ingress:
    - ports:
      - protocol: TCP
        port: 8000
---
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: bwd-tls-ingress
spec:
  backend:
    serviceName: bwd-nodeport
    servicePort: 80
  tls:
    - secretName: bwd-tls
---
# There need to be separate services and ingresses for darklang.com
# so that we can serve with the correct TLS key
kind: Service
apiVersion: v1
metadata:
  name: darklang-nodeport
spec:
  type: NodePort
  selector:
    app: bwd-app
  ports:
    - name: darklang-nodeport-port
      protocol: TCP
      port: 80
      targetPort: http-proxy-port
---
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: darklang-tls-ingress
spec:
  backend:
    serviceName: darklang-nodeport
    servicePort: 80
  tls:
    - secretName: darklang-tls
