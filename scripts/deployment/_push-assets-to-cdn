#!/usr/bin/env python3.8

import glob
import os
import sys
import json
import subprocess
import tempfile
import shutil
import time

ROOT_OF_FILES = "backend/static/"
BUCKET = "gs://darklang-static-assets"

# This script's goal is to gather all of our static assets in `backend/static/`
# and upload them to GCP via `gsutil cp`, with the correct Content-Type. Some
# files should maintain their exact name (e.g. vendor assets), and others
# should have their contents' hash injected into their name, as expected by
# `etags.json`.
#
# As we want to upload with the correct content types, which `gsutil cp` allows
# via a `-h "Content-Type: {mimetype}"` argument, we must make a call to the
# tool once per Content-Type. We must also ensure we maintain relative file
# paths when uploaded. (i.e. backend/static/blazor/blazor.js should end up at
# {BUCKET}/blazor/blazor-HASH.js).
#
# Inconveniently, `gsutil cp` doesn't provide a way to pass in a list of
# (local path/name, remote path/name) pairs, making it tricky to do rename
# transforms. We get around this by creating a temp folder per content-type
# and rather than providing a list of files, we provide such a directory, and
# include the `-r` (recursive) argument, along with the Content-Type
#
# The algorithm of this script is:
# - gather a list of all files in `backend/static/`
# - exclude some of them (by name, or extension, or otherwise)
# - create a temp directory
# - copy all relevant files from `backend/static/` to the appropriate
#   Content-Type -specific subfolder of our temp directory, renaming
#   some files to have the hashes from etags.json in their file name
# - call `gsutil cp -r` per Content-Type -specific directory


# Exists because we can't make folders named like "application/json"
def encode_dirname_with_slash(dirname):
  return dirname.replace("/", "#slash#")


# Gather list of files in known dir; filter out files we don't care about
def should_copy_file(file_name):
  files_to_skip = [f"{ROOT_OF_FILES}.gitkeep", f"{ROOT_OF_FILES}etags.json"]
  if file_name in files_to_skip:
    return False

  if file_name.endswith(".gz") or file_name.endswith(".br"):
    return False

  return True


static_files_to_copy = [
    f for f in glob.glob(f"{ROOT_OF_FILES}**", recursive=True)
    if os.path.isfile(f) and should_copy_file(f)
]


# Determine content-type / mimetype
# See also ApiServer.configureStaticContent
def mime_type_for(file_name):
  # Common web stuff
  if file_name.endswith(".css"): return "text/css"
  elif file_name.endswith(".js"): return "application/javascript"
  elif file_name.endswith(".json"): return "application/json"
  elif file_name.endswith(".txt"): return "text/plain"
  elif file_name.endswith(".png"): return "image/png"
  elif file_name.endswith(".svg"): return "image/svg+xml"
  elif file_name.endswith(".html"):
    return "text/html"
    # Fonts
  elif file_name.endswith(".ttf"):
    return "font/ttf"
  elif file_name.endswith(".woff"):
    return "font/woff"
  elif file_name.endswith(".woff2"):
    return "font/woff2"
  elif file_name.endswith(".eot"):
    return "application/vnd.ms-fontobject"
    # Blazor
  elif file_name.endswith(".wasm"):
    return "application/wasm"
  elif file_name.endswith(".pdb"):
    return "text/plain"
  elif file_name.endswith(".dll"):
    return "application/octet-stream"
  elif file_name.endswith(".dat"):
    return "application/octet-stream"
  elif file_name.endswith(".blat"):
    return "application/octet-stream"
  else:
    # Don't allow anything else
    print(f'Unknown extension for {file_name}')
    sys.exit(-1)


# Parse `etags.json` to extract expected file content hashes, which we will
# inject into filenames
with open(f"{ROOT_OF_FILES}/etags.json", 'r') as f:
  etags = json.load(f)


def should_hash(filename):
  return not filename.startswith("vendor/")


# Create a local temp directory, with a subfolder per Content-Type relevant,
# and copy our files to such, respecting relative location.
# We'll later call `gsutil cp` per-folder with a `-r` (recursive) flag.
temp_dir = tempfile.gettempdir() + "/static-assets"
shutil.rmtree(temp_dir, ignore_errors=True)


# inserts the has of the file's contents (per etags.js) into the file name,
# if relevant
def get_remote_file_name(file_path):
  file_path = file_path.replace(ROOT_OF_FILES, '')

  if should_hash(file_path):
    file_hash = etags[file_path]
    (base, _dot, extension) = file_path.rpartition('.')
    return f'{base}-{file_hash}.{extension}'
  else:
    return file_path


def get_mimetype_temp_dir(mimetype):
  mimetype_folder = encode_dirname_with_slash(mimetype)
  return f'{temp_dir}/{mimetype_folder}'


mimetypes_to_process = set()
for f in static_files_to_copy:
  mimetype = mime_type_for(f)
  mimetypes_to_process.add(mimetype)
  target_filename = get_remote_file_name(f)
  target = f'{get_mimetype_temp_dir(mimetype)}/{target_filename}'

  # make sure subfolders are there so the copy doesn't fail
  os.makedirs(os.path.dirname(target), exist_ok=True)

  shutil.copy(f, target)

# Copy the files to CDN - one `gsutil cp` call per Content-Type
for mimetype in mimetypes_to_process:
  mimetype_subfolder = get_mimetype_temp_dir(mimetype)
  # Prepare a `gsutil cp` call
  # - global gsutil args
  #   -h: set header
  #   -m: Upload assets in parallel, within call
  # - `cp`-specific args
  #   -Z: Uploaded file is served as zipped. Also adds 'no-transform' to Cache-Control header
  #   -n: Don't overwrite
  #   -r: Copy recursively
  start = time.time()
  gsutil_command = f'''gsutil \
    -h "Content-Type:{mimetype}" \
    -h "Cache-Control:public" \
    -m \
    cp -Z -n -r . "{BUCKET}"
    '''

  result = subprocess.run(gsutil_command, shell=True, cwd=mimetype_subfolder, capture_output=True)
  # Note: checking stderr won't work here, as gsutil sends non-error logs to
  # stderr by default. We could add a `-q` flag to avoid this, but the logs are
  # useful in local debugging, so let's check the returncode instead.
  # https://github.com/GoogleCloudPlatform/gsutil/issues/116#issuecomment-13929972
  if result.returncode != 0:
    print(f"Failed to upload {mimetype_subfolder} to CDN bucket {BUCKET}. Failed with {result.returncode} {result.stderr}.")
    sys.exit(1)

  elapsed = (time.time() - start)
  print(f"Uploaded dir {mimetype_subfolder} for mimetype {mimetype} in {elapsed}s")

# Wrap up
shutil.rmtree(temp_dir, ignore_errors=True)
print("Done!")