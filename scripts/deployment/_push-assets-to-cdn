#!/usr/bin/env python3.10

import glob
import os
import sys
import json
import subprocess
import tempfile
import shutil
import time

ROOT_OF_FILES = "backend/static/"
BUCKET = "gs://darklang-static-assets"

# This script's goal is to gather all of our static assets in `backend/static/` and
# upload them to GCP using gcloud, with the correct Content-Type. Some files should
# maintain their exact name (e.g. vendor assets), and others should have their
# contents' hash injected into their name, as expected by `etags.json`.
#
# As we want to upload with the correct content types, which `gcloud storage cp`
# allows, we must make a call to the tool once per Content-Type. We must also ensure
# we maintain relative file paths when uploaded. (i.e.
# backend/static/blazor/blazor.js should end up at {BUCKET}/blazor/blazor-HASH.js).
#
# TODO: reevaluate now that we use `gcloud storage cp`
# Inconveniently, `gsutil cp` doesn't provide a way to pass in a list of (local
# path/name, remote path/name) pairs, making it tricky to do rename transforms. We
# get around this by creating a temp folder per content-type and rather than
# providing a list of files, we provide such a directory, and include the `-r`
# (recursive) argument, along with the Content-Type
#
# The algorithm of this script is:
# - gather a list of all files in `backend/static/`
# - exclude some of them (by name, or extension, or otherwise)
# - create a temp directory
# - copy all relevant files from `backend/static/` to the appropriate
#   Content-Type -specific subfolder of our temp directory, renaming
#   some files to have the hashes from etags.json in their file name
# - call `gcloud storage cp -r` per Content-Type -specific directory


# Exists because we can't make folders named like "application/json"
def encode_dirname_with_slash(dirname):
  return dirname.replace("/", "#slash#")


# Gather list of files in known dir; filter out files we don't care about
def should_copy_file(file_name):
  files_to_skip = [f"{ROOT_OF_FILES}.gitkeep", f"{ROOT_OF_FILES}etags.json"]
  if file_name in files_to_skip:
    return False

  if file_name.endswith(".gz") or file_name.endswith(".br"):
    return False

  return True


static_files_to_copy = [
    f for f in glob.glob(f"{ROOT_OF_FILES}**", recursive=True)
    if os.path.isfile(f) and should_copy_file(f)
]


# Determine content-type / mimetype
# See also ApiServer.configureStaticContent
def mime_type_for(file_name):
  # Common web stuff
  if file_name.endswith(".css"): return "text/css"
  elif file_name.endswith(".js"): return "application/javascript"
  elif file_name.endswith(".json"): return "application/json"
  elif file_name.endswith(".txt"): return "text/plain"
  elif file_name.endswith(".png"): return "image/png"
  elif file_name.endswith(".svg"): return "image/svg+xml"
  elif file_name.endswith(".html"):
    return "text/html"
    # Fonts
  elif file_name.endswith(".ttf"):
    return "font/ttf"
  elif file_name.endswith(".woff"):
    return "font/woff"
  elif file_name.endswith(".woff2"):
    return "font/woff2"
  elif file_name.endswith(".eot"):
    return "application/vnd.ms-fontobject"
    # Blazor
  elif file_name.endswith(".pdb"):
    return "text/plain"
  elif file_name.endswith(".dll"):
    return "application/octet-stream"
  elif file_name.endswith(".dat"):
    return "application/octet-stream"
  elif file_name.endswith(".blat"):
    return "application/octet-stream"
  else:
    # Don't allow anything else
    print(f'Unknown extension for {file_name}')
    sys.exit(-1)


# Parse `etags.json` to extract expected file content hashes, which we will
# inject into filenames
with open(f"{ROOT_OF_FILES}/etags.json", 'r') as f:
  etags = json.load(f)


def should_hash(filename):
  return not filename.startswith("vendor/")


# Create a local temp directory, with a subfolder per Content-Type relevant,
# and copy our files to such, respecting relative location.
# We'll later call `gcloud storage cp` per-folder with a `-r` (recursive) flag.
temp_dir = tempfile.gettempdir() + "/static-assets"
shutil.rmtree(temp_dir, ignore_errors=True)


# inserts the has of the file's contents (per etags.js) into the file name,
# if relevant
def get_remote_file_name(file_path):
  file_path = file_path.replace(ROOT_OF_FILES, '')

  if should_hash(file_path):
    file_hash = etags[file_path]
    (base, _dot, extension) = file_path.rpartition('.')
    return f'{base}-{file_hash}.{extension}'
  else:
    return file_path


def get_mimetype_temp_dir(mimetype):
  mimetype_folder = encode_dirname_with_slash(mimetype)
  return f'{temp_dir}/{mimetype_folder}'


mimetypes_to_process = set()
for f in static_files_to_copy:
  mimetype = mime_type_for(f)
  mimetypes_to_process.add(mimetype)
  target_filename = get_remote_file_name(f)
  target = f'{get_mimetype_temp_dir(mimetype)}/{target_filename}'

  # make sure subfolders are there so the copy doesn't fail
  os.makedirs(os.path.dirname(target), exist_ok=True)

  shutil.copy(f, target)

# Copy the files to CDN - one `gcloud storage cp` call per Content-Type
for mimetype in mimetypes_to_process:
  mimetype_subfolder = get_mimetype_temp_dir(mimetype)
  start = time.time()

  # Unfortunately, gzip-local overrides the cache-control header, so this doesn't work
  # See discussion in https://issuetracker.google.com/issues/252956852
  # So let's just stick to gsutil for now
  # A better option in future is to use dynamic compression: https://cloud.google.com/cdn/docs/dynamic-compression
  # command = f'''gcloud storage cp \
  #   --content-type {mimetype} \
  #   --cache-control "public, no-transform" \
  #   --gzip-local \
  #   --no-clobber \
  #   --recursive \
  #   . "{BUCKET}"
  #   '''

  # - global gsutil args
  #   -h: set header
  #   -m: Upload assets in parallel, within call
  # - `cp`-specific args
  #   -Z: Uploaded file is served as zipped. Also adds 'no-transform' to Cache-Control header
  #   -n: Don't overwrite
  #   -r: Copy recursively
  start = time.time()
  command = f'''gsutil \
    -h "Content-Type:{mimetype}" \
    -h "Cache-Control:public" \
    -m \
    cp -Z -n -r . "{BUCKET}"
  '''

  result = subprocess.run(command,
                          shell=True,
                          cwd=mimetype_subfolder,
                          capture_output=True)
  if result.returncode != 0:
    print(
        f"Failed to upload {mimetype_subfolder} to CDN bucket {BUCKET}. Failed with {result.returncode} {result.stderr}."
    )
    sys.exit(1)

  elapsed = (time.time() - start)
  print(f"Uploaded dir {mimetype_subfolder} for mimetype {mimetype} in {elapsed}s")

# Wrap up
shutil.rmtree(temp_dir, ignore_errors=True)
print("Done!")
